"""Simplified model comparison - only lightweight models."""

from __future__ import annotations

import sys
import time
from pathlib import Path

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import site
site.addsitedir(site.getusersitepackages())

from src.config import default_medqa_experiment
from src.training.supervised import SupervisedExperiment
from src.utils import set_seed, save_experiment_results

# Import test script functions
import importlib.util
test_module_path = project_root / "scripts" / "test_model_examples.py"
spec = importlib.util.spec_from_file_location("test_model_examples", test_module_path)
test_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(test_module)
test_model_on_examples = test_module.test_model_on_examples
create_test_examples = test_module.create_test_examples

from src.models.student import StudentMCQAModel
from src.utils import get_device
import torch


# Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ»ĞµĞ³ĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ
MODELS_TO_TEST = [
    ("distilbert-base-uncased", "DistilBERT"),
    # ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ»ĞµĞ³ĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾
]


def create_config_for_model(model_name: str, max_samples: int = 150, num_epochs: int = 2):
    """Create config for specific model."""
    config = default_medqa_experiment(
        experiment_name=f"compare_{model_name.replace('/', '_')}",
        experiment_type="supervised",
    )
    
    config.model.student_model_name = model_name
    config.dataset.max_samples = max_samples
    config.training.batch_size = 8  # ĞœĞµĞ½ÑŒÑˆĞµ Ğ´Ğ»Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸
    config.training.num_epochs = num_epochs
    config.training.warmup_steps = 5
    config.training.eval_steps = 10
    config.training.logging_steps = 5
    config.training.num_threads = 4
    config.model.device = "cpu"
    
    return config


def train_and_evaluate_model(model_name: str, model_display: str, max_samples: int = 150, num_epochs: int = 2):
    """Train a model and return results."""
    print("=" * 80)
    print(f"ğŸ¤– ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ•: {model_display} ({model_name})")
    print("=" * 80)
    print()
    
    start_time = time.time()
    
    # Create config
    config = create_config_for_model(model_name, max_samples, num_epochs)
    config.validate()
    
    print(f"ğŸ“‹ ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹:")
    print(f"  - Ğ”Ğ°Ğ½Ğ½Ñ‹Ñ…: {config.dataset.max_samples} Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²")
    print(f"  - Ğ­Ğ¿Ğ¾Ñ…: {config.training.num_epochs}")
    print(f"  - Batch size: {config.training.batch_size}")
    print()
    
    # Set seed
    set_seed(config.training.seed, num_threads=config.training.num_threads)
    
    # Train
    print("ğŸ‹ï¸ ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ...")
    train_start = time.time()
    experiment = SupervisedExperiment(config)
    experiment.train()
    train_time = time.time() - train_start
    print(f"âœ“ ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: {train_time:.1f}Ñ ({train_time/60:.1f} Ğ¼Ğ¸Ğ½)")
    print()
    
    # Evaluate
    print("ğŸ“Š ĞÑ†ĞµĞ½ĞºĞ°...")
    eval_start = time.time()
    final_metrics = experiment.evaluate()
    eval_time = time.time() - eval_start
    print(f"âœ“ ĞÑ†ĞµĞ½ĞºĞ°: {eval_time:.1f}Ñ")
    print()
    
    # Test on examples
    print("ğŸ§ª Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ...")
    test_start = time.time()
    test_accuracy = 0.0
    
    model_path = Path(config.training.output_dir) / config.experiment_name / "model.pt"
    if model_path.exists():
        try:
            from src.utils import load_config
            config_path = model_path.parent / "config.json"
            if config_path.exists():
                experiment_config = load_config(config_path)
                model = StudentMCQAModel(experiment_config.model)
                
                checkpoint = torch.load(model_path, map_location=get_device("cpu"))
                if isinstance(checkpoint, dict) and "model_state_dict" in checkpoint:
                    model.load_state_dict(checkpoint["model_state_dict"])
                else:
                    model.load_state_dict(checkpoint)
                
                device = get_device("cpu")
                model.to(device)
                model.eval()
                
                examples = create_test_examples()
                test_results = test_model_on_examples(
                    model=model,
                    examples=examples,
                    device=device,
                    max_examples=6,
                )
                test_accuracy = test_results['accuracy']
        except Exception as e:
            print(f"âš  ĞÑˆĞ¸Ğ±ĞºĞ° Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: {e}")
    
    test_time = time.time() - test_start
    total_time = time.time() - start_time
    
    # Save results
    save_experiment_results(
        experiment_name=config.experiment_name,
        config=config,
        final_metrics=final_metrics,
        training_metrics=experiment.training_metrics,
    )
    
    print(f"âœ“ Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: {test_time:.1f}Ñ")
    print()
    
    return {
        "model_name": model_display,
        "model_path": model_name,
        "train_time": train_time,
        "eval_time": eval_time,
        "test_time": test_time,
        "total_time": total_time,
        "accuracy": final_metrics.get("accuracy", 0.0),
        "expected_correctness": final_metrics.get("expected_correctness", 0.0),
        "test_accuracy": test_accuracy,
    }


def main():
    """Compare models."""
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘     Ğ¡Ğ ĞĞ’ĞĞ•ĞĞ˜Ğ• ĞœĞĞ”Ğ•Ğ›Ğ•Ğ™ (ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ)                   â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()
    
    overall_start = time.time()
    results = []
    
    max_samples = 150
    num_epochs = 2
    
    print(f"ğŸ“Š ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹:")
    print(f"  - Ğ”Ğ°Ğ½Ğ½Ñ‹Ñ…: {max_samples} Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²")
    print(f"  - Ğ­Ğ¿Ğ¾Ñ…: {num_epochs}")
    print(f"  - ĞœĞ¾Ğ´ĞµĞ»ĞµĞ¹: {len(MODELS_TO_TEST)}")
    print()
    
    # Train each model
    for i, (model_name, model_display) in enumerate(MODELS_TO_TEST, 1):
        print(f"\n{'='*80}")
        print(f"ĞœĞĞ”Ğ•Ğ›Ğ¬ {i}/{len(MODELS_TO_TEST)}")
        print(f"{'='*80}\n")
        
        try:
            result = train_and_evaluate_model(model_name, model_display, max_samples, num_epochs)
            results.append(result)
        except Exception as e:
            print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {e}")
            import traceback
            traceback.print_exc()
            results.append({
                "model_name": model_display,
                "error": str(e),
            })
    
    # Print comparison
    overall_time = time.time() - overall_start
    
    print("\n" + "=" * 80)
    print("ğŸ“Š Ğ¡Ğ ĞĞ’ĞĞ•ĞĞ˜Ğ• Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢ĞĞ’")
    print("=" * 80)
    print()
    
    print(f"{'ĞœĞ¾Ğ´ĞµĞ»ÑŒ':<20} {'Accuracy':<12} {'Exp. Corr.':<12} {'Test Acc.':<12} {'Ğ’Ñ€ĞµĞ¼Ñ':<12}")
    print("-" * 80)
    
    for result in results:
        if "error" in result:
            print(f"{result['model_name']:<20} {'ERROR':<12} {'-':<12} {'-':<12} {'-':<12}")
        else:
            print(
                f"{result['model_name']:<20} "
                f"{result['accuracy']:.4f}      "
                f"{result['expected_correctness']:.4f}      "
                f"{result['test_accuracy']:.4f}      "
                f"{result['total_time']:.1f}Ñ"
            )
    
    print("-" * 80)
    print(f"{'Ğ˜Ğ¢ĞĞ“Ğ':<20} {'-':<12} {'-':<12} {'-':<12} {overall_time:.1f}Ñ")
    print()
    
    if results and "error" not in results[0]:
        best = max([r for r in results if "error" not in r], key=lambda x: x['accuracy'])
        print(f"ğŸ† Ğ›ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ: {best['model_name']} (accuracy: {best['accuracy']:.4f})")
        print()
    
    print(f"â±ï¸  ĞĞ±Ñ‰ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ: {overall_time:.1f}Ñ ({overall_time/60:.1f} Ğ¼Ğ¸Ğ½)")
    
    if overall_time > 600:
        print("âš  ĞŸĞ Ğ•Ğ’Ğ«Ğ¨Ğ•Ğ Ğ›Ğ˜ĞœĞ˜Ğ¢ 10 ĞœĞ˜ĞĞ£Ğ¢!")
    else:
        print("âœ… Ğ£Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ÑÑŒ Ğ² Ğ»Ğ¸Ğ¼Ğ¸Ñ‚!")
    
    return results


if __name__ == "__main__":
    try:
        results = main()
        print("\nâœ… Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾!")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


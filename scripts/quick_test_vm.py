"""Quick test script for VM - trains and tests in under 10 minutes."""

from __future__ import annotations

import sys
import time
from pathlib import Path

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.config import default_medqa_experiment
from src.training.supervised import SupervisedExperiment
from src.utils import set_seed, save_experiment_results

# Import test script functions
import importlib.util
test_module_path = project_root / "scripts" / "test_model_examples.py"
spec = importlib.util.spec_from_file_location("test_model_examples", test_module_path)
test_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(test_module)
test_model_on_examples = test_module.test_model_on_examples
create_test_examples = test_module.create_test_examples

from src.models.student import StudentMCQAModel
from src.utils import get_device
import torch


def quick_test_config():
    """Create config optimized for quick testing (< 10 minutes)."""
    config = default_medqa_experiment(
        experiment_name="quick_test_vm",
        experiment_type="supervised",
    )
    
    # Minimal settings for quick test
    config.dataset.max_samples = 100  # Very small dataset
    config.training.batch_size = 16  # Reasonable batch size
    config.training.num_epochs = 1  # Just 1 epoch
    config.training.warmup_steps = 5  # Minimal warmup
    config.training.eval_steps = 20  # Evaluate often
    config.training.logging_steps = 5  # Log often
    config.training.num_threads = 4  # Use more threads on VM
    
    # Use CPU (no GPU needed for quick test)
    config.model.device = "cpu"
    
    return config


def main():
    """Run quick test: train and evaluate in under 10 minutes."""
    start_time = time.time()
    print("=" * 80)
    print("üöÄ –ë–´–°–¢–†–´–ô –¢–ï–°–¢ –ù–ê VM (< 10 –º–∏–Ω—É—Ç)")
    print("=" * 80)
    print()
    
    # Create config
    print("üìã –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏...")
    config = quick_test_config()
    config.validate()
    print(f"‚úì –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞: {config.experiment_name}")
    print(f"  - –î–∞–Ω–Ω—ã—Ö: {config.dataset.max_samples} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"  - –≠–ø–æ—Ö: {config.training.num_epochs}")
    print(f"  - Batch size: {config.training.batch_size}")
    print()
    
    # Set seed
    set_seed(config.training.seed, num_threads=config.training.num_threads)
    
    # Train
    print("üèãÔ∏è –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    train_start = time.time()
    experiment = SupervisedExperiment(config)
    experiment.train()
    train_time = time.time() - train_start
    print(f"‚úì –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {train_time:.1f} —Å–µ–∫—É–Ω–¥ ({train_time/60:.1f} –º–∏–Ω—É—Ç)")
    print()
    
    # Quick evaluation
    print("üìä –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏...")
    eval_start = time.time()
    final_metrics = experiment.evaluate()
    eval_time = time.time() - eval_start
    print(f"‚úì –û—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {eval_time:.1f} —Å–µ–∫—É–Ω–¥")
    print()
    
    # Print metrics
    print("=" * 80)
    print("üìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø")
    print("=" * 80)
    for metric_name, metric_value in final_metrics.items():
        print(f"  {metric_name}: {metric_value:.4f}")
    print("=" * 80)
    print()
    
    # Test on examples
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö...")
    test_start = time.time()
    
    # Get model
    model_path = Path(config.training.output_dir) / config.experiment_name / "model.pt"
    if not model_path.exists():
        print(f"‚ö† –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
        return
    
    # Load model for testing
    from src.utils import load_config
    config_path = model_path.parent / "config.json"
    if config_path.exists():
        experiment_config = load_config(config_path)
        model = StudentMCQAModel(experiment_config.model)
        
        checkpoint = torch.load(model_path, map_location=get_device("cpu"))
        if isinstance(checkpoint, dict) and "model_state_dict" in checkpoint:
            model.load_state_dict(checkpoint["model_state_dict"])
        else:
            model.load_state_dict(checkpoint)
        
        device = get_device("cpu")
        model.to(device)
        model.eval()
        
        # Test on examples
        examples = create_test_examples()
        test_results = test_model_on_examples(
            model=model,
            examples=examples,
            device=device,
            max_examples=6,
        )
    else:
        print("‚ö† Config –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö")
        test_results = {"total": 0, "correct": 0, "accuracy": 0.0}
    test_time = time.time() - test_start
    
    print(f"‚úì –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {test_time:.1f} —Å–µ–∫—É–Ω–¥")
    print()
    
    # Print test summary
    print("=" * 80)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    print("=" * 80)
    print(f"–í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {test_results['total']}")
    print(f"–ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö: {test_results['correct']}")
    print(f"–¢–æ—á–Ω–æ—Å—Ç—å: {test_results['accuracy']:.2%}")
    print("=" * 80)
    print()
    
    # Save results
    results_file = save_experiment_results(
        experiment_name=config.experiment_name,
        config=config,
        final_metrics=final_metrics,
        training_metrics=experiment.training_metrics,
    )
    print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_file}")
    print()
    
    # Total time
    total_time = time.time() - start_time
    print("=" * 80)
    print(f"‚è±Ô∏è –û–ë–©–ï–ï –í–†–ï–ú–Ø: {total_time:.1f} —Å–µ–∫—É–Ω–¥ ({total_time/60:.1f} –º–∏–Ω—É—Ç)")
    print("=" * 80)
    
    if total_time > 600:  # 10 minutes
        print("‚ö† –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –í—Ä–µ–º—è –ø—Ä–µ–≤—ã—Å–∏–ª–æ 10 –º–∏–Ω—É—Ç!")
    else:
        print("‚úÖ –£–ª–æ–∂–∏–ª–∏—Å—å –≤ –ª–∏–º–∏—Ç 10 –º–∏–Ω—É—Ç!")
    
    return {
        "total_time": total_time,
        "train_time": train_time,
        "eval_time": eval_time,
        "test_time": test_time,
        "metrics": final_metrics,
        "test_accuracy": test_results['accuracy'],
    }


if __name__ == "__main__":
    try:
        results = main()
        if results:
            print("\n‚úÖ –¢–µ—Å—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!")
            sys.exit(0)
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


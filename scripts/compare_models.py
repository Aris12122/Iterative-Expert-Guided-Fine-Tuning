"""Script to train and compare different model architectures."""

from __future__ import annotations

import sys
import time
from pathlib import Path

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Add user site-packages to path
import site
site.addsitedir(site.getusersitepackages())

from src.config import default_medqa_experiment
from src.training.supervised import SupervisedExperiment
from src.utils import set_seed, save_experiment_results

# Import test script functions
import importlib.util
test_module_path = project_root / "scripts" / "test_model_examples.py"
spec = importlib.util.spec_from_file_location("test_model_examples", test_module_path)
test_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(test_module)
test_model_on_examples = test_module.test_model_on_examples
create_test_examples = test_module.create_test_examples

from src.models.student import StudentMCQAModel
from src.utils import get_device
import torch


# –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (—Ç–æ–ª—å–∫–æ –ª–µ–≥–∫–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞)
MODELS_TO_TEST = [
    "distilbert-base-uncased",  # –ú–∞–ª–µ–Ω—å–∫–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è
    # "bert-base-uncased",      # –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞
    # "roberta-base",           # –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞
]


def create_config_for_model(model_name: str, max_samples: int = 200, num_epochs: int = 3):
    """Create config for specific model."""
    config = default_medqa_experiment(
        experiment_name=f"compare_{model_name.replace('/', '_')}",
        experiment_type="supervised",
    )
    
    config.model.student_model_name = model_name
    config.dataset.max_samples = max_samples
    config.training.batch_size = 8  # –£–º–µ–Ω—å—à–µ–Ω –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    config.training.num_epochs = num_epochs
    config.training.warmup_steps = 10
    config.training.eval_steps = 15
    config.training.logging_steps = 5
    config.training.num_threads = 4
    config.model.device = "cpu"
    
    return config


def train_and_evaluate_model(model_name: str, max_samples: int = 200, num_epochs: int = 3):
    """Train a model and return results."""
    print("=" * 80)
    print(f"ü§ñ –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò: {model_name}")
    print("=" * 80)
    print()
    
    start_time = time.time()
    
    # Create config
    config = create_config_for_model(model_name, max_samples, num_epochs)
    config.validate()
    
    print(f"üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
    print(f"  - –ú–æ–¥–µ–ª—å: {model_name}")
    print(f"  - –î–∞–Ω–Ω—ã—Ö: {config.dataset.max_samples} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"  - –≠–ø–æ—Ö: {config.training.num_epochs}")
    print(f"  - Batch size: {config.training.batch_size}")
    print()
    
    # Set seed
    set_seed(config.training.seed, num_threads=config.training.num_threads)
    
    # Train
    print("üèãÔ∏è –û–±—É—á–µ–Ω–∏–µ...")
    train_start = time.time()
    experiment = SupervisedExperiment(config)
    experiment.train()
    train_time = time.time() - train_start
    print(f"‚úì –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {train_time:.1f} —Å–µ–∫—É–Ω–¥ ({train_time/60:.1f} –º–∏–Ω—É—Ç)")
    print()
    
    # Evaluate
    print("üìä –û—Ü–µ–Ω–∫–∞...")
    eval_start = time.time()
    final_metrics = experiment.evaluate()
    eval_time = time.time() - eval_start
    print(f"‚úì –û—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {eval_time:.1f} —Å–µ–∫—É–Ω–¥")
    print()
    
    # Test on examples
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö...")
    test_start = time.time()
    
    model_path = Path(config.training.output_dir) / config.experiment_name / "model.pt"
    test_accuracy = 0.0
    
    if model_path.exists():
        try:
            from src.utils import load_config
            config_path = model_path.parent / "config.json"
            if config_path.exists():
                experiment_config = load_config(config_path)
                model = StudentMCQAModel(experiment_config.model)
                
                checkpoint = torch.load(model_path, map_location=get_device("cpu"))
                if isinstance(checkpoint, dict) and "model_state_dict" in checkpoint:
                    model.load_state_dict(checkpoint["model_state_dict"])
                else:
                    model.load_state_dict(checkpoint)
                
                device = get_device("cpu")
                model.to(device)
                model.eval()
                
                examples = create_test_examples()
                test_results = test_model_on_examples(
                    model=model,
                    examples=examples,
                    device=device,
                    max_examples=6,
                )
                test_accuracy = test_results['accuracy']
        except Exception as e:
            print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
    
    test_time = time.time() - test_start
    total_time = time.time() - start_time
    
    # Save results
    save_experiment_results(
        experiment_name=config.experiment_name,
        config=config,
        final_metrics=final_metrics,
        training_metrics=experiment.training_metrics,
    )
    
    print(f"‚úì –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {test_time:.1f} —Å–µ–∫—É–Ω–¥")
    print()
    
    return {
        "model_name": model_name,
        "train_time": train_time,
        "eval_time": eval_time,
        "test_time": test_time,
        "total_time": total_time,
        "accuracy": final_metrics.get("accuracy", 0.0),
        "expected_correctness": final_metrics.get("expected_correctness", 0.0),
        "test_accuracy": test_accuracy,
    }


def main():
    """Compare multiple models."""
    print("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó")
    print("‚ïë     –°–†–ê–í–ù–ï–ù–ò–ï –†–ê–ó–ù–´–• –ú–û–î–ï–õ–ï–ô                                 ‚ïë")
    print("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù")
    print()
    
    overall_start = time.time()
    results = []
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞ (—É–º–µ–Ω—å—à–µ–Ω—ã –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –≤—Ä–µ–º–µ–Ω–∏)
    max_samples = 150  # –ù–µ–º–Ω–æ–≥–æ —É–º–µ–Ω—å—à–µ–Ω–æ
    num_epochs = 2  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    
    print(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–µ—Å—Ç–∞:")
    print(f"  - –î–∞–Ω–Ω—ã—Ö: {max_samples} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"  - –≠–ø–æ—Ö: {num_epochs}")
    print(f"  - –ú–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–µ—Å—Ç–∞: {len(MODELS_TO_TEST)}")
    print()
    
    # Train each model
    for i, model_name in enumerate(MODELS_TO_TEST, 1):
        print(f"\n{'='*80}")
        print(f"–ú–û–î–ï–õ–¨ {i}/{len(MODELS_TO_TEST)}")
        print(f"{'='*80}\n")
        
        try:
            result = train_and_evaluate_model(model_name, max_samples, num_epochs)
            results.append(result)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ {model_name}: {e}")
            import traceback
            traceback.print_exc()
            results.append({
                "model_name": model_name,
                "error": str(e),
            })
    
    # Print comparison
    overall_time = time.time() - overall_start
    
    print("\n" + "=" * 80)
    print("üìä –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("=" * 80)
    print()
    
    # Table header
    print(f"{'–ú–æ–¥–µ–ª—å':<30} {'Accuracy':<12} {'Exp. Corr.':<12} {'Test Acc.':<12} {'–í—Ä–µ–º—è':<12}")
    print("-" * 80)
    
    # Table rows
    for result in results:
        if "error" in result:
            print(f"{result['model_name']:<30} {'ERROR':<12} {'-':<12} {'-':<12} {'-':<12}")
        else:
            print(
                f"{result['model_name']:<30} "
                f"{result['accuracy']:.4f}      "
                f"{result['expected_correctness']:.4f}      "
                f"{result['test_accuracy']:.4f}      "
                f"{result['total_time']:.1f}—Å"
            )
    
    print("-" * 80)
    print(f"{'–ò–¢–û–ì–û':<30} {'-':<12} {'-':<12} {'-':<12} {overall_time:.1f}—Å")
    print()
    
    # Find best model
    valid_results = [r for r in results if "error" not in r]
    if valid_results:
        best_accuracy = max(valid_results, key=lambda x: x['accuracy'])
        best_test = max(valid_results, key=lambda x: x['test_accuracy'])
        fastest = min(valid_results, key=lambda x: x['total_time'])
        
        print("üèÜ –õ–£–ß–®–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(f"  –õ—É—á—à–∞—è accuracy: {best_accuracy['model_name']} ({best_accuracy['accuracy']:.4f})")
        print(f"  –õ—É—á—à–∞—è test accuracy: {best_test['model_name']} ({best_test['test_accuracy']:.4f})")
        print(f"  –°–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è: {fastest['model_name']} ({fastest['total_time']:.1f}—Å)")
        print()
    
    print(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {overall_time:.1f} —Å–µ–∫—É–Ω–¥ ({overall_time/60:.1f} –º–∏–Ω—É—Ç)")
    
    if overall_time > 600:
        print("‚ö† –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –í—Ä–µ–º—è –ø—Ä–µ–≤—ã—Å–∏–ª–æ 10 –º–∏–Ω—É—Ç!")
    else:
        print("‚úÖ –£–ª–æ–∂–∏–ª–∏—Å—å –≤ –ª–∏–º–∏—Ç 10 –º–∏–Ω—É—Ç!")
    
    return results


if __name__ == "__main__":
    try:
        results = main()
        print("\n‚úÖ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        sys.exit(0)
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


# Диагностический отчет: Почему accuracy ~0.25?

## Проблема
Accuracy получается примерно 0.25, что соответствует случайному угадыванию для 4 классов (25%).

## Проверка данных ✅

Данные загружаются **правильно**:
- Метки распределены нормально: {0: 288, 1: 245, 2: 250, 3: 217} (из 1000 примеров)
- Нормализация меток работает корректно
- Все метки в диапазоне [0, 3]

## Возможные причины низкой accuracy

### 1. ⚠️ **Слишком мало данных для обучения**

**Текущие настройки:**
- `max_samples=500` - всего 500 примеров для обучения
- После разбиения на train/dev/test остается еще меньше

**Решение:**
```python
# В config.py или через CLI:
--max_samples None  # Использовать весь датасет (182822 примеров)
```

### 2. ⚠️ **Слишком мало эпох**

**Текущие настройки:**
- `num_epochs=1` - всего 1 эпоха обучения
- Модель не успевает обучиться

**Решение:**
```python
--num_epochs 5  # Минимум 3-5 эпох для нормального обучения
```

### 3. ⚠️ **Слишком маленький batch_size**

**Текущие настройки:**
- `batch_size=8` - очень маленький батч
- Градиенты могут быть нестабильными

**Решение:**
```python
--batch_size 32  # Для CPU можно 16-32, для GPU 32-64
```

### 4. ⚠️ **Слишком мало warmup_steps**

**Текущие настройки:**
- `warmup_steps=10` - очень мало для стабильного обучения

**Решение:**
```python
--warmup_steps 500  # Для полного датасета нужно больше
```

## Рекомендации для улучшения результатов

### Быстрый тест (для проверки кода):
```bash
python3 scripts/train_supervised.py \
    --experiment_name "test_quick" \
    --max_samples 1000 \
    --batch_size 16 \
    --num_epochs 3 \
    --warmup_steps 100
```

### Полное обучение (для реальных результатов):
```bash
python3 scripts/train_supervised.py \
    --experiment_name "supervised_full" \
    --max_samples None \
    --batch_size 32 \
    --num_epochs 5 \
    --warmup_steps 500
```

## Проверка обучения

Чтобы убедиться, что модель обучается, проверьте:

1. **Loss должен уменьшаться:**
   - В начале: loss ~1.4 (для 4 классов: -log(1/4) ≈ 1.39)
   - После обучения: loss должен быть < 1.0

2. **Accuracy должна расти:**
   - В начале: ~0.25 (случайное угадывание)
   - После обучения: должна быть > 0.4-0.5 (минимум)

3. **Проверьте логи:**
   ```bash
   tail -f outputs/logs/<experiment_name>.log
   ```

## Вывод

**Код работает правильно**, но текущие настройки (500 примеров, 1 эпоха, batch_size=8) недостаточны для нормального обучения.

**Для получения хороших результатов нужно:**
1. ✅ Увеличить количество данных (`max_samples=None`)
2. ✅ Увеличить количество эпох (`num_epochs=5`)
3. ✅ Увеличить batch_size (`batch_size=32`)
4. ✅ Увеличить warmup_steps (`warmup_steps=500`)

После этих изменений accuracy должна быть значительно выше 0.25.

